{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fa1113-e1c9-42fa-ad27-c2af1b609659",
   "metadata": {},
   "source": [
    "## Model Selection Rationale: Siamese Preference Transformer\n",
    "The core task is a Pairwise Comparison Problem where the goal is to predict the preference between two financial time-series sequences (A and B). So I adopted a Siamese Preference Network architecture, which is ideally suited for learning the similarity or relational metrics between two distinct inputs.\n",
    "\n",
    "Initial experiments with only Base Features showed low accuracy. To enhance the model's generalization ability, a systematic feature engineering process was executed. I used various combinations of features using Profitability/Duration (Log_Return, Proxy_Return, Time_in_Position), Momentum/Trend (MACD, RSI), and Volatility (ATR, Hist_Vol)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10568b61-54f8-46de-8201-340c39294335",
   "metadata": {},
   "source": [
    "## Model Settings\n",
    "0. Base\n",
    "1. 0 + log return + proxy return\n",
    "2. 1 + time in position\n",
    "3. 2 + Momentum/Trend (MACD,MACD Signal,RSI)\n",
    "4. 3 + Volatility (ATR,Historical Volatility)\n",
    "5. 1 + Volatility (ATR,Historical Volatility)\n",
    "6. 1 + N_LAYERS (4->6)\n",
    "7. 6 + MODEL_DIM (256->512)\n",
    "8. 6 + MODEL_DIM (256) + N_LAYERS (6->8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e294f-a58a-4b56-83af-5a381525af61",
   "metadata": {},
   "source": [
    "## Best Setting with Public Accuracy = 0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c5a2d-9c77-469a-bd36-2bb7cab5179c",
   "metadata": {},
   "source": [
    "1. Features: Open, High, Low, Close, Volume, Position, Log Return, Proxy Return\n",
    "2. MODEL_DIM = 256\n",
    "3. N_LAYERS = 8\n",
    "4. N_HEADS = 4\n",
    "5. DROPOUT_RATE = 0.1\n",
    "6. LEARNING_RATE = 1e-4\n",
    "7. BATCH_SIZE = 32\n",
    "8. EPOCHS = 50 with (PATIENCE = 10 Early Stopping by val loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc164d-fc5c-4547-a031-d82596f5bb7a",
   "metadata": {},
   "source": [
    "Model 8 showed best public accuracy = 0.81, while Model 6 with the lowest val loss showed public accuracy = 0.77. This validates the principle that for this financial time-series problem, generalization capability (Decision Boundary) is more critical than the model's confidence level (Loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4d6cd-5d71-4fb0-ab49-6581dd780b8b",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad37d6-0871-4608-9009-e49457a08fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(DEVICE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c1522-9b27-4142-ab71-04d4504218db",
   "metadata": {},
   "source": [
    "## Initial Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fee221-b456-44f0-ab5d-2ba1d81fc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 4\n",
    "MODEL_DIM = 256\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "DROPOUT_RATE = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE=42\n",
    "TEST_SIZE=0.2\n",
    "NUM_EPOCHS=50\n",
    "PATIENCE=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a581b1-d1a6-4553-be04-ba294ed4d049",
   "metadata": {},
   "source": [
    "## PrepareDataset for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9d44a-131c-413f-b548-d91d791b6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset(Dataset):\n",
    "    def __init__(self, df, base_path):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        \n",
    "        # Check whether \"label\" column exists\n",
    "        self.has_labels = 'label' in df.columns\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        pair_id = row['id']\n",
    "        \n",
    "        try:\n",
    "            # Load raw sequence files\n",
    "            file_a = f\"{self.base_path}sample_{pair_id}_a.csv\"\n",
    "            file_b = f\"{self.base_path}sample_{pair_id}_b.csv\"\n",
    "            \n",
    "            data_a = pd.read_csv(file_a, sep='\\s+', header=None, on_bad_lines='skip')\n",
    "            data_b = pd.read_csv(file_b, sep='\\s+', header=None, on_bad_lines='skip')\n",
    "\n",
    "            # CRITICAL: Apply preprocessing pipeline\n",
    "            data_a_processed = preprocess_sequence(data_a.copy())\n",
    "            data_b_processed = preprocess_sequence(data_b.copy())\n",
    "            \n",
    "            seq_a = torch.tensor(data_a_processed, dtype=torch.float32)\n",
    "            seq_b = torch.tensor(data_b_processed, dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sequences for ID {pair_id}: {e}\")\n",
    "            # When failed to load sequence, return empty tensors so collate_fn can handle it\n",
    "            return torch.empty(0), torch.empty(0), -1 \n",
    "\n",
    "        # Handle label existence (train/test mode)\n",
    "        if self.has_labels:\n",
    "            label = torch.tensor([row['label']], dtype=torch.float32)\n",
    "            # Training mode: return (A, B, label)\n",
    "            return seq_a, seq_b, label\n",
    "        else:\n",
    "            # Test mode: return (A, B, id)\n",
    "            return seq_a, seq_b, pair_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6872a9-1316-4caf-9e9b-70c64af08760",
   "metadata": {},
   "source": [
    "## Sequence Encoder and Siamese Network for Transformer Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617c1f2-a991-4790-a35a-6250978eab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, model_dim=MODEL_DIM, n_heads=N_HEADS, n_layers=N_LAYERS):\n",
    "        super().__init__()\n",
    "        self.model_dim = MODEL_DIM\n",
    "        \n",
    "        # 1. Map input dimension to model_dim\n",
    "        self.input_layer = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # 2. Define the Transformer Encoder Layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=model_dim * 4,\n",
    "            # Assuming DROPOUT_RATE is defined\n",
    "            dropout=DROPOUT_RATE, \n",
    "            batch_first=True # Order is (Batch, Seq, Feature)\n",
    "        )\n",
    "        \n",
    "        # 3. Define the Transformer Encoder Stack\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch Size, Sequence Length, Input Dim)\n",
    "        \n",
    "        # Map input dimension\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through the Transformer Encoder\n",
    "        output = self.transformer_encoder(x)\n",
    "        \n",
    "        # Final Embedding: A vector representing the entire sequence (Pooling)\n",
    "        # 1. Mean Pooling: Take the average along the sequence length dimension\n",
    "        embedding = output.mean(dim=1) \n",
    "        # embedding shape: (Batch Size, Model Dim)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "class SiamesePreferenceModel(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, model_dim=MODEL_DIM):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the identical encoder with shared weights\n",
    "        self.encoder = SequenceEncoder(input_dim, model_dim)\n",
    "        \n",
    "        # Comparison Head (Receives z_A, z_B, and |z_A - z_B| as input)\n",
    "        # Input dimension: 3 * model_dim\n",
    "        head_input_dim = 3 * model_dim \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(head_input_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(model_dim, model_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim // 2, 1) # Final output is 1 (probability)\n",
    "        )\n",
    "        \n",
    "    def forward(self, seq_a, seq_b):\n",
    "        # Pass sequences A and B through the encoder to generate embeddings (shared weights)\n",
    "        z_a = self.encoder(seq_a)\n",
    "        z_b = self.encoder(seq_b)\n",
    "        \n",
    "        # Construct the feature vector for comparison\n",
    "        diff = torch.abs(z_a - z_b) # Absolute difference\n",
    "        \n",
    "        # Concatenation: [z_a, z_b, |z_a - z_b|]\n",
    "        comparison_vector = torch.cat((z_a, z_b, diff), dim=1)\n",
    "        \n",
    "        # Pass through the classifier and apply Sigmoid\n",
    "        logits = self.classifier(comparison_vector)\n",
    "        prediction = torch.sigmoid(logits)\n",
    "        \n",
    "        # prediction shape: (Batch Size, 1)\n",
    "        return prediction, z_a, z_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630ea2f-b334-4778-a14a-06d338a991fe",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afc0b6-0f1a-488d-8bd2-989ef4fd0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path='model_best.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self._save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self._save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def _save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296aba5f-8dcc-4793-893a-58d440c4136a",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c506b-7f21-4da4-bb10-8dbca7541f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, num_epochs=50, patience=10, model_save_path='model_best.pth'):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Early Stopping Init\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=model_save_path)\n",
    "    \n",
    "    print(\"--- Start Training ---\")\n",
    "    print(f\"Layers: {N_LAYERS} | Model Dim: {MODEL_DIM} | Fixed LR: {LEARNING_RATE} | Save Path: {model_save_path}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        total_train_loss = 0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # 1. Training Step\n",
    "        for seq_a, seq_b, labels in train_dataloader:\n",
    "            if len(labels) == 0:\n",
    "                continue\n",
    "            \n",
    "            seq_a, seq_b, labels = seq_a.to(DEVICE), seq_b.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions, _, _ = model(seq_a, seq_b)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            predicted_classes = (predictions >= 0.5).float()\n",
    "            correct_train_predictions += (predicted_classes == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        # calculate train_accuracy and loss\n",
    "        avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else 0\n",
    "        train_accuracy = correct_train_predictions / total_train_samples if total_train_samples > 0 else 0\n",
    "\n",
    "        # 2. Validation Step\n",
    "        val_loss, val_accuracy = validate_model(model, val_dataloader, criterion)\n",
    "        \n",
    "        # 3. Print output and check Early Stopping\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early Stopping by val_loss\n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"--- Early Stopping: Stop training, no more decrese in validation loss ---\")\n",
    "            break\n",
    "\n",
    "    # 4. Load the best model setting\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(f\"--- Training Complete ('{model_save_path}' Loaded) ---\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd016ca1-e601-4d88-b080-315d5aa03740",
   "metadata": {},
   "source": [
    "## Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964950b-50df-4c80-8218-3ac98c36335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion):\n",
    "    model.eval() \n",
    "    total_val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_a, seq_b, labels in dataloader:\n",
    "            if len(labels) == 0:\n",
    "                continue\n",
    "            \n",
    "            seq_a, seq_b, labels = seq_a.to(DEVICE), seq_b.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            predictions, _, _ = model(seq_a, seq_b)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_val_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            predicted_classes = (predictions >= 0.5).float()\n",
    "            correct_predictions += (predicted_classes == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    avg_loss = total_val_loss / total_samples if total_samples > 0 else 0\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    model.train() \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee9abd-22ce-4b78-9478-0305a35789ed",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Example dataset for (seq_a, seq_b, label) pairs using selected features.\n",
    "    raw_pairs: list of (df_a, df_b, label)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_pairs, selected_features):\n",
    "        self.raw_pairs = raw_pairs\n",
    "        self.selected_features = selected_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df_a, df_b, label = self.raw_pairs[idx]\n",
    "\n",
    "        seq_a = preprocess_sequence(df_a, self.selected_features)\n",
    "        seq_b = preprocess_sequence(df_b, self.selected_features)\n",
    "\n",
    "        # You may need to convert to torch tensors in collate_fn or here\n",
    "        return seq_a, seq_b, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72eadf1-8ca1-4fa4-bf78-b33520f1501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "pair_ids = train_df['id'].unique()\n",
    "train_ids, val_ids = train_test_split(\n",
    "    pair_ids, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa112622-81e2-4234-88f3-f1fc71fe233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_80 = train_df[train_df['id'].isin(train_ids)].reset_index(drop=True)\n",
    "val_df_20 = train_df[train_df['id'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total Sample: {len(train_df)}\")\n",
    "print(f\"Training Sample (80%): {len(train_df_80)}\")\n",
    "print(f\"Validation Sample (20%): {len(val_df_20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2bde7-dc3c-4526-a8a8-27e6e92a711b",
   "metadata": {},
   "source": [
    "## collate_fn() for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf8dca-3d74-4d16-b9e3-437f538edd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[2] != -1]\n",
    "    if not batch:\n",
    "        return torch.empty(0), torch.empty(0), [] \n",
    "\n",
    "    # 1. Split Sequence and Label/ID\n",
    "    seqs_a = [item[0] for item in batch]\n",
    "    seqs_b = [item[1] for item in batch]\n",
    "    labels_or_ids = [item[2] for item in batch]\n",
    "\n",
    "    # 2. Sequence Padding\n",
    "    padded_seqs_a = pad_sequence(seqs_a, batch_first=True, padding_value=0.0)\n",
    "    padded_seqs_b = pad_sequence(seqs_b, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # 3. Label/ID\n",
    "    if torch.is_tensor(labels_or_ids[0]) and labels_or_ids[0].dtype == torch.float32:\n",
    "        labels_batch = torch.cat(labels_or_ids, dim=0).view(-1, 1)\n",
    "        return padded_seqs_a, padded_seqs_b, labels_batch\n",
    "    \n",
    "    # 3.2. Test Data (If item[2] is scalar ID)\n",
    "    else:\n",
    "        return padded_seqs_a, padded_seqs_b, labels_or_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95248c-f9a3-4848-a582-440c082c3c51",
   "metadata": {},
   "source": [
    "## Preprocess_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5d555-3709-4343-ac5b-359643eb2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 3. Z-Score Standardisation\n",
    "    features_to_scale = ['Open', 'High', 'Low', 'Close', 'Volume'] \n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        \n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0 \n",
    "\n",
    "    # 4. Final Features\n",
    "    final_features = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Position']].values\n",
    "    \n",
    "    return final_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916b89c-b361-421e-9662-120b6f7e7926",
   "metadata": {},
   "source": [
    "## Checking Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7c73d-fec2-43f0-b208-c8d112af820c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def check_class_distribution(train_df):\n",
    "    label_counts = train_df['label'].value_counts()\n",
    "    total_samples = len(train_df)\n",
    "\n",
    "    print(f\"Total sample: {total_samples}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    distribution_data = []\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        meaning = \"A wins (label 0)\" if label == 0 else \"B wins (label 1)\"\n",
    "        distribution_data.append([label, meaning, count, f\"{percentage:.2f}%\"])\n",
    "\n",
    "    distribution_df = pd.DataFrame(distribution_data, \n",
    "                                   columns=['label', 'meaning', 'count', 'ratio'])\n",
    "    \n",
    "    print(distribution_df.to_markdown(index=False))\n",
    "\n",
    "    if len(label_counts) == 2:\n",
    "        ratio = label_counts.max() / label_counts.min()\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "check_class_distribution(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871916a9-5dab-4c97-ab25-3284f504f58a",
   "metadata": {},
   "source": [
    "## Model 0: Train basic model with Initial Parameters and set-ups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcbfe5-ed3f-450e-8ac0-27e97083b9c0",
   "metadata": {},
   "source": [
    "Epoch 14/50 | Train Loss: 0.6554 | Train Acc: 0.6145 | Val Loss: 0.6575 | Val Acc: 0.5952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9620b-cfd4-4b3f-bbba-d6ea1917787b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = PrepareDataset(train_df_80, base_path='./train/')\n",
    "val_dataset = PrepareDataset(val_df_20, base_path='./train/')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "INPUT_DIM = 6\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_0 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_0.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f39e15-b93d-44cc-954a-288112d54542",
   "metadata": {},
   "source": [
    "## Model 1: Log_Return and Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b32697-5fb9-4725-b74d-0b2a4c9ced7c",
   "metadata": {},
   "source": [
    "Train Loss: 0.4665 | Train Acc: 0.7711 | Val Loss: 0.4987 | Val Acc: 0.7619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706eea75-bf40-4cd2-ae21-20c3b07f4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 5. Z-Score Standardisation\n",
    "    features_to_scale = ['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume'] \n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        \n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0 \n",
    "\n",
    "    # 6. Final Features\n",
    "    final_features = df[['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position']].values\n",
    "    \n",
    "    return final_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ad869-aabd-4505-8756-0c5ce71647cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 8\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_1 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_1.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547e429-420b-4e80-8ad1-e703868a5f24",
   "metadata": {},
   "source": [
    "## Model 2: Add Time-in-Position Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f595b7-245b-42df-89f0-3bf8b144f987",
   "metadata": {},
   "source": [
    "Epoch 24/50 | Train Loss: 0.4069 | Train Acc: 0.8253 | Val Loss: 0.5338 | Val Acc: 0.7619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bc5c9-ebd6-49cc-ac01-389cbbbbdb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Time-in-Position\n",
    "    df.loc[:, 'Time_in_Position'] = 0\n",
    "    current_position_count = 0\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        current_Position = df.iloc[i]['Position']\n",
    "        \n",
    "        if current_Position == 1:\n",
    "            current_position_count += 1\n",
    "        else:\n",
    "            current_position_count = 0\n",
    "        \n",
    "        df.iloc[i, df.columns.get_loc('Time_in_Position')] = current_position_count\n",
    "\n",
    "    # 5. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 6. Z-Score Standardisation\n",
    "    features_to_scale = ['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Time_in_Position'] \n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        \n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0 \n",
    "\n",
    "    # 7. Final Features\n",
    "    final_features = df[['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position', 'Time_in_Position']].values\n",
    "    \n",
    "    return final_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b37850-45aa-4c1e-bd04-af45454b4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 9\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_2 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_2.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8781d0-65b7-448a-90f7-c1bc98709e60",
   "metadata": {},
   "source": [
    "## Model 3: add MACD, MACD_Signal, RSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1418308-44b3-4e81-af9d-95b878de12fe",
   "metadata": {},
   "source": [
    "Epoch 16/50 | Train Loss: 0.6743 | Train Acc: 0.6084 | Val Loss: 0.6629 | Val Acc: 0.5952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff60c68-87a2-4a0b-b621-c0da83bbdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Time-in-Position\n",
    "    df.loc[:, 'Time_in_Position'] = 0\n",
    "    current_position_count = 0\n",
    "    \n",
    "    time_in_position_column_index = df.columns.get_loc('Time_in_Position')\n",
    "    Position_column_index = df.columns.get_loc('Position')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_Position = df.iloc[i, Position_column_index]\n",
    "        \n",
    "        if current_Position == 1:\n",
    "            current_position_count += 1\n",
    "        else:\n",
    "            current_position_count = 0\n",
    "            \n",
    "        df.iloc[i, time_in_position_column_index] = current_position_count\n",
    "        \n",
    "    # 5. MACD & RSI\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df.loc[:, 'MACD'] = exp1 - exp2\n",
    "    df.loc[:, 'MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean() # Signal Line\n",
    "\n",
    "    # RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).ewm(span=14, adjust=False).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).ewm(span=14, adjust=False).mean()\n",
    "    # rsê°€ 0ìœ¼ë¡œ ë‚˜ë‰˜ëŠ” ê²ƒì„ ë°©ì§€\n",
    "    rs = np.divide(gain, loss.replace(0, np.nan)) \n",
    "    df.loc[:, 'RSI'] = 100 - (100 / (1 + rs))\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # 6. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 7. Z-Score Standardisation\n",
    "    features_to_scale = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Time_in_Position', \n",
    "        'MACD', 'MACD_Signal', 'RSI'\n",
    "    ]\n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        \n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0\n",
    "\n",
    "    # 8. Final Features\n",
    "    final_features = df[[\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position', 'Time_in_Position',\n",
    "        'MACD', 'MACD_Signal', 'RSI'\n",
    "    ]].values\n",
    "    \n",
    "    return final_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e81b20-1857-49c8-8a44-ac9981635186",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 12\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_3 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_3.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d418c-6e17-4dc1-9567-e7d5a7b91c87",
   "metadata": {},
   "source": [
    "## Model 4: Add ATR, Historical Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70e31b-ea79-4f01-97ad-df1f0eb285ae",
   "metadata": {},
   "source": [
    "Train Loss: 0.5668 | Train Acc: 0.7771 | Val Loss: 0.6097 | Val Acc: 0.6667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dd0fc-32d6-461b-a6ad-8b65be9ee4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Time-in-Position\n",
    "    df.loc[:, 'Time_in_Position'] = 0\n",
    "    current_position_count = 0\n",
    "    \n",
    "    time_in_position_column_index = df.columns.get_loc('Time_in_Position')\n",
    "    Position_column_index = df.columns.get_loc('Position')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_Position = df.iloc[i, Position_column_index]\n",
    "        if current_Position == 1:\n",
    "            current_position_count += 1\n",
    "        else:\n",
    "            current_position_count = 0\n",
    "        df.iloc[i, time_in_position_column_index] = current_position_count\n",
    "\n",
    "    # 5. MACD & RSI\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df.loc[:, 'MACD'] = exp1 - exp2\n",
    "    df.loc[:, 'MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean() \n",
    "\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).ewm(span=14, adjust=False).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).ewm(span=14, adjust=False).mean()\n",
    "    rs = np.divide(gain, loss.replace(0, np.nan)) \n",
    "    df.loc[:, 'RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # 6. ATR & Historical Volatility\n",
    "    \n",
    "    # True Range (TR)\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift(1))\n",
    "    # TR: max(H-L, |H-C_prev|, |L-C_prev|)\n",
    "    df.loc[:, 'TR'] = high_low.combine(high_close, max).combine(low_close, max)\n",
    "\n",
    "    # Average True Range (ATR) - 14-period EMA\n",
    "    df.loc[:, 'ATR'] = df['TR'].ewm(span=14, adjust=False).mean()\n",
    "    \n",
    "    # Historical Volatility (HV) - 14-period Rolling Std Dev of Log Returns\n",
    "    df.loc[:, 'Historical_Volatility'] = df['Log_Return'].rolling(window=14).std()\n",
    "\n",
    "    # 7. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 8. Z-Score Standardisation \n",
    "    features_to_scale = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Time_in_Position', \n",
    "        'MACD', 'MACD_Signal', 'RSI', 'ATR', 'Historical_Volatility' \n",
    "    ]\n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0\n",
    "\n",
    "    # 9. Final Features\n",
    "    final_features = df[[\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position', 'Time_in_Position',\n",
    "        'MACD', 'MACD_Signal', 'RSI', 'ATR', 'Historical_Volatility' \n",
    "    ]].values\n",
    "    \n",
    "    return final_features.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d3467-647c-4ba2-92c2-05a4c178952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 14\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_4 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_4.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d43bb4-8c05-455b-97dd-622daf405c95",
   "metadata": {},
   "source": [
    "## Model 5: Exclude MACD & RSI, Add ATR, Historical Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c49439-afd7-4d04-a2ed-d24a590e3add",
   "metadata": {},
   "source": [
    "Train Loss: 0.6885 | Train Acc: 0.5964 | Val Loss: 0.6905 | Val Acc: 0.5238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f00113-2317-49c5-a10e-21d9a86be73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Time-in-Position\n",
    "    df.loc[:, 'Time_in_Position'] = 0\n",
    "    current_position_count = 0\n",
    "    \n",
    "    time_in_position_column_index = df.columns.get_loc('Time_in_Position')\n",
    "    Position_column_index = df.columns.get_loc('Position')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_Position = df.iloc[i, Position_column_index]\n",
    "        if current_Position == 1:\n",
    "            current_position_count += 1\n",
    "        else:\n",
    "            current_position_count = 0\n",
    "        df.iloc[i, time_in_position_column_index] = current_position_count\n",
    "\n",
    "    # 6. ATR & Historical Volatility\n",
    "    \n",
    "    # True Range (TR)\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift(1))\n",
    "    # TR: max(H-L, |H-C_prev|, |L-C_prev|)\n",
    "    df.loc[:, 'TR'] = high_low.combine(high_close, max).combine(low_close, max)\n",
    "\n",
    "    # Average True Range (ATR) - 14-period EMA\n",
    "    df.loc[:, 'ATR'] = df['TR'].ewm(span=14, adjust=False).mean()\n",
    "    \n",
    "    # Historical Volatility (HV) - 14-period Rolling Std Dev of Log Returns\n",
    "    df.loc[:, 'Historical_Volatility'] = df['Log_Return'].rolling(window=14).std()\n",
    "\n",
    "    # 7. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 8. Z-Score Standardisation \n",
    "    features_to_scale = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Time_in_Position', \n",
    "        'ATR', 'Historical_Volatility' \n",
    "    ]\n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0\n",
    "\n",
    "    # 9. Final Features\n",
    "    final_features = df[[\n",
    "        'Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position', 'Time_in_Position',\n",
    "        'ATR', 'Historical_Volatility' \n",
    "    ]].values\n",
    "    \n",
    "    return final_features.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b07d7-2446-441a-a2f4-bb5e061fdfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 11\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "\n",
    "trained_model_5 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_5.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48396ee3-1804-4874-8eb6-7e0f7174d9c8",
   "metadata": {},
   "source": [
    "## Model 6: Features = Base + Log Return + Proxy, NUM_LAYERS = 4 -> 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea7458-04b9-41a4-b72b-ed45c1ee361f",
   "metadata": {},
   "source": [
    "Train Loss: 0.3107 | Train Acc: 0.8795 | Val Loss: 0.4977 | Val Acc: 0.7381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645d940-8d8d-4116-89d0-0306c2fdd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(df: pd.DataFrame) -> np.ndarray:\n",
    "    # 1. OHLCV + Position\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Position']\n",
    "\n",
    "    # 2. Log Return\n",
    "    df.loc[:, 'Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # 3. Proxy Return\n",
    "    df.loc[:, 'Proxy_Return'] = df['Position'] * df['Log_Return']\n",
    "\n",
    "    # 4. Remove NaN\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # 5. Z-Score Standardisation\n",
    "    features_to_scale = ['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume'] \n",
    "    \n",
    "    for feature in features_to_scale:\n",
    "        mean = df[feature].mean()\n",
    "        std = df[feature].std()\n",
    "        \n",
    "        if std != 0:\n",
    "            df.loc[:, feature] = (df[feature] - mean) / std\n",
    "        else:\n",
    "            df.loc[:, feature] = 0 \n",
    "\n",
    "    # 6. Final Features\n",
    "    final_features = df[['Open', 'High', 'Low', 'Close', 'Log_Return', 'Proxy_Return', 'Volume', 'Position']].values\n",
    "    \n",
    "    return final_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d22aa-0538-49f8-92b7-dd25846e2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 6\n",
    "INPUT_DIM = 8\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "trained_model_6 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_6.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32020ffb-3e0c-45a5-9561-62c455509cd4",
   "metadata": {},
   "source": [
    "## Model 7: MODEL_DIM = 256 -> 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b10a44-aee8-4e9b-b814-890d33aaa2ee",
   "metadata": {},
   "source": [
    "Train Loss: 0.6589 | Train Acc: 0.6506 | Val Loss: 0.6684 | Val Acc: 0.5714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9a716-4407-41bd-8277-13ff4b26ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 6\n",
    "INPUT_DIM = 8\n",
    "MODEL_DIM = 512\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "trained_model_7 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_7.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cd83a-6197-4535-afd0-d5e0d86422ba",
   "metadata": {},
   "source": [
    "## Model 8: MODEL_DIM = 256, N_LAYERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe97df-02f5-4bae-83e1-7d199b4bc19a",
   "metadata": {},
   "source": [
    "Train Loss: 0.4262 | Train Acc: 0.8373 | Val Loss: 0.5296 | Val Acc: 0.7857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200079e-79a0-4b5b-87a1-ff2447c99716",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 8\n",
    "INPUT_DIM = 8\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "trained_model_8 = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    num_epochs=NUM_EPOCHS, \n",
    "    patience=PATIENCE,\n",
    "    model_save_path=\"trained_model_8.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3f246-1791-48b8-a34d-17d9398e6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_submission(model, dataloader, model_path):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(f\"Loaded model weights from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ðŸš¨ FATAL ERROR: Can't find '{model_path}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸš¨ ERROR loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_a, seq_b, ids in dataloader:\n",
    "            \n",
    "            if len(ids) == 0:\n",
    "                continue\n",
    "                \n",
    "            seq_a = seq_a.to(DEVICE)\n",
    "            seq_b = seq_b.to(DEVICE)\n",
    "\n",
    "            probabilities, _, _ = model(seq_a, seq_b)\n",
    "            \n",
    "            final_predictions = (probabilities > 0.5).int()\n",
    "            \n",
    "            predictions.extend(final_predictions.cpu().numpy().flatten())\n",
    "            row_ids.extend(ids) \n",
    "\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': row_ids, \n",
    "        'pred': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df['id'] = submission_df['id'].astype(int)\n",
    "    submission_df = submission_df.sort_values(by='id').reset_index(drop=True)\n",
    "    \n",
    "    submission_file_path = 'submission.csv'\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"File '{submission_file_path}' generated.\")\n",
    "    print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61008d1f-3499-469a-938d-c8b418449a62",
   "metadata": {},
   "source": [
    "## Model 6: Public score 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de5f99-8b5b-4acf-83ca-ba3b67e20295",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "test_df = pd.read_csv('test.csv') \n",
    "test_dataset = PrepareDataset(test_df, base_path='./test/')\n",
    "TEST_BATCH_SIZE = 32 \n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "predict_submission(model, test_dataloader, model_path='trained_model_6.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7994fb-c9da-4a80-a55a-8440edc34b12",
   "metadata": {},
   "source": [
    "## Model 8: Public Score 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0830-22ad-4f89-8d03-3d3bd06e042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = 8\n",
    "INPUT_DIM = 8\n",
    "MODEL_DIM = 256\n",
    "model = SiamesePreferenceModel(input_dim=INPUT_DIM, model_dim=MODEL_DIM)\n",
    "model.to(DEVICE) \n",
    "test_df = pd.read_csv('test.csv') \n",
    "test_dataset = PrepareDataset(test_df, base_path='./test/')\n",
    "TEST_BATCH_SIZE = 32 \n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "predict_submission(model, test_dataloader, model_path='trained_model_8.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
